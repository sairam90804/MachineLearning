# -*- coding: utf-8 -*-
"""Assignment 3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12gXRDQTSSGH1DjfKLhn8Caifz9F-rfam
"""

#**********************Question 1***************************
#**********************Question 1 a)************************
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.impute import SimpleImputer

# Extracting features and target variable
features = ['CAR_TYPE', 'OCCUPATION', 'EDUCATION']
target = 'CAR_USE'

# Filter out missing values
train_data = data[features + [target]].dropna()

# Recode the EDUCATION categories into numeric values
education_mapping = {'Below High School': 0, 'High School': 1, 'Bachelors': 2, 'Masters': 3, 'Doctors': 4}
train_data['EDUCATION'] = train_data['EDUCATION'].map(education_mapping)

# Encode categorical features using LabelEncoder
label_encoders = {}
for feature in ['CAR_TYPE', 'OCCUPATION']:
    le = LabelEncoder()
    train_data[feature] = le.fit_transform(train_data[feature])
    label_encoders[feature] = le

# Split the data into training and testing sets
X = train_data[features]
y = train_data[target]

# Handle missing values
imputer = SimpleImputer(strategy='most_frequent')
X = imputer.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train the decision tree model
tree_model = DecisionTreeClassifier(max_depth=2, criterion='entropy', random_state=42)
tree_model.fit(X_train, y_train)

#**********************Question 1 b)************************
import pandas as pd

# Create a DataFrame for the fictitious person's attributes
fictitious_person_data = pd.DataFrame({
    'CAR_TYPE': ['Minivan'],
    'OCCUPATION': ['STEM'],
    'EDUCATION': ['Masters']
})

# Assume 'car_type_encoder' and 'data' are available

# Encode the CAR_TYPE variable
fictitious_person_data['CAR_TYPE'] = car_type_encoder.transform(fictitious_person_data['CAR_TYPE'])

# Map education levels to numerical values
education_mapping = {'Below High School': 0, 'High School': 1, 'Bachelors': 2, 'Masters': 3, 'Doctors': 4}
fictitious_person_data['EDUCATION'] = fictitious_person_data['EDUCATION'].map(education_mapping)

# Encode the OCCUPATION variable
fictitious_person_data['OCCUPATION'] = car_type_encoder.transform(fictitious_person_data['OCCUPATION'])

# Predicting Car Usage probabilities
predicted_probabilities_fictitious_person = clf.predict_proba(fictitious_person_data)[0]
print(f"\nCar Usage Probabilities for Fictitious Person (STEM, Masters, Minivan): {predicted_probabilities_fictitious_person}")

#**********************Question 1 c)************************
# Fictitious person attributes
occupation_fictitious = 'Student'
education_fictitious = 'High School'
car_type_fictitious = 'Pickup'

# Encode occupation and education
occupation_fictitious_encoded = car_type_encoder.transform([occupation_fictitious])[0]
education_fictitious_encoded = education_mapping[education_fictitious]

# Create a DataFrame for the fictitious person's attributes
fictitious_person_data = pd.DataFrame({
    'CAR_TYPE': [car_type_fictitious],
    'OCCUPATION': [occupation_fictitious_encoded],
    'EDUCATION': [education_fictitious_encoded]
})

# Predicting Car Usage probabilities for the fictitious person
predicted_probabilities_fictitious_person = clf.predict_proba(fictitious_person_data)[0]
print("Car Usage Probabilities for Fictitious Person (Student, High School, Pickup):")
print("Commercial:", predicted_probabilities_fictitious_person[1])
print("Private:", predicted_probabilities_fictitious_person[0])

#**********************Question 1 d)************************
import matplotlib.pyplot as plt

# Predict probabilities for CAR_USE = Private
probs_private = clf.predict_proba(X_test)[:, 0]  # Probabilities for Private

# Generate a histogram of predicted probabilities for CAR_USE = Private
plt.hist(probs_private, bins=np.arange(0, 1.05, 0.05), alpha=0.7, color='blue', label='Private')
plt.xlabel('Predicted Probabilities for CAR_USE = Private')
plt.ylabel('Proportion of Observations')
plt.legend()
plt.show()

#**********************Question 1 e)************************
# Calculate the misclassification rate
predicted_labels = clf.predict(X_test)
misclassification_rate = 1 - accuracy_score(y_test, predicted_labels)
print('Misclassification rate:', misclassification_rate)

#**********************Question 2************************

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score

from google.colab import files
uploaded = files.upload()
import io

# Load the data
data =  pd.read_excel(io.BytesIO(uploaded['claim_history.xlsx']))

# Drop rows with missing values in the specified predictors
data.dropna(subset=['CAR_USE', 'CAR_TYPE', 'OCCUPATION', 'EDUCATION'], inplace=True)

# Encode categorical variables using LabelEncoder
label_encoder = LabelEncoder()
data['CAR_USE'] = label_encoder.fit_transform(data['CAR_USE'])
data['CAR_TYPE'] = label_encoder.fit_transform(data['CAR_TYPE'])
data['OCCUPATION'] = label_encoder.fit_transform(data['OCCUPATION'])
data['EDUCATION'] = label_encoder.fit_transform(data['EDUCATION'])

# Define predictors and target variable
X = data[['CAR_TYPE', 'OCCUPATION', 'EDUCATION']]
y = data['CAR_USE']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train the NaÃ¯ve Bayes model with Laplace smoothing (alpha=0.01)
naive_bayes_model = MultinomialNB(alpha=0.01)
naive_bayes_model.fit(X_train, y_train)

# Predict probabilities for both classes
probabilities = naive_bayes_model.predict_proba(X_test)

# Class probabilities
class_prob_commercial = probabilities[:, 1].mean()  # 1 corresponds to 'Commercial'
class_prob_private = probabilities[:, 0].mean()  # 0 corresponds to 'Private'

print('Class Probabilities:')
print('P(Commercial) =', class_prob_commercial)
print('P(Private) =', class_prob_private)

#**********************Question 2 b)************************
# Cross-tabulate CAR_USE by CAR_TYPE
car_type_tab = pd.crosstab(data['CAR_USE'], data['CAR_TYPE'], margins=True)

# Cross-tabulate CAR_USE by OCCUPATION
occupation_tab = pd.crosstab(data['CAR_USE'], data['OCCUPATION'], margins=True)

# Cross-tabulate CAR_USE by EDUCATION
education_tab = pd.crosstab(data['CAR_USE'], data['EDUCATION'], margins=True)

print('Cross-tabulation of CAR_USE by CAR_TYPE:')
print(car_type_tab)
print('\nCross-tabulation of CAR_USE by OCCUPATION:')
print(occupation_tab)
print('\nCross-tabulation of CAR_USE by EDUCATION:')
print(education_tab)

#**********************Question 2 c)************************
# Encode the features for the fictitious person
management_below_hs_sports_car = label_encoder.transform(['Management', 'Below High School', 'Sports Car']).reshape(1, -1)

# Predict probabilities for the fictitious person
fictitious_person2_probabilities = naive_bayes_model.predict_proba(management_below_hs_sports_car)

# Class probabilities for the fictitious person
fictitious_person2_prob_commercial = fictitious_person2_probabilities[:, label_encoder.transform(['Commercial'])].item()
fictitious_person2_prob_private = fictitious_person2_probabilities[:, label_encoder.transform(['Private'])].item()

print('Car Usage Probabilities for Fictitious Person 2:')
print('P(Commercial | Management, Below High School, Sports Car) =', fictitious_person2_prob_commercial)
print('P(Private | Management, Below High School, Sports Car) =', fictitious_person2_prob_private)

#**********************Question 2 d) 2 e)************************
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

# Load the dataset


# Drop rows with missing values in the specified columns


# Select relevant features and label
features = ['CAR_TYPE', 'OCCUPATION', 'EDUCATION']
label = 'CAR_USE'

# Encode categorical variables
data_encoded = data.copy()
for feature in features + [label]:
    data_encoded[feature] = pd.Categorical(data_encoded[feature])
    data_encoded[feature] = data_encoded[feature].cat.codes

# Split the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(data_encoded[features], data_encoded[label], test_size=0.2, random_state=42)

# Train a decision tree classifier
clf = DecisionTreeClassifier(random_state=42)
clf.fit(X_train, y_train)

# Predict probabilities for CAR_USE = Private
probs_private = clf.predict_proba(X_test)[:, 0]  # Probabilities for Private
probs_commercial = clf.predict_proba(X_test)[:, 1]  # Probabilities for Commercial

# Generate a histogram of predicted probabilities for CAR_USE = Private
plt.hist(probs_private, bins=np.arange(0, 1.05, 0.05), alpha=0.7, color='blue', label='Private')
plt.xlabel('Predicted Probabilities for CAR_USE = Private')
plt.ylabel('Proportion of Observations')
plt.legend()
plt.show()

# Calculate the misclassification rate
predicted_labels = np.where(probs_private > 0.5, 0, 1)  # Predict Private if prob > 0.5, else Commercial
misclassification_rate = 1 - accuracy_score(y_test, predicted_labels)
print('Misclassification rate:', misclassification_rate)

