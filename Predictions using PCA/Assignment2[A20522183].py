# -*- coding: utf-8 -*-
"""Assignment 2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hCJJg6Y6qCdIoorFbn_n4cGlsOdQJbQy

# Assignment 2 - CS 484 Introduction to Machine Learning
## CWID : A20522183
"""

#***********************************Question 1********************************
import pandas as pd
from mlxtend.preprocessing import TransactionEncoder
from mlxtend.frequent_patterns import apriori, association_rules

def calculate_metrics(association_rules_result):
    confidence = association_rules_result['confidence'].values[0]
    support_soda = association_rules_result['antecedent support'].values[0]
    lift = association_rules_result['lift'].values[0]
    leverage = association_rules_result['leverage'].values[0]
    zhang = (confidence - support_soda) / max(support_soda, 1 - support_soda)

    return confidence, lift, leverage, zhang

# Friend items
friend_items = {
    'Andrew': ['Cheese', 'Cracker', 'Soda', 'Wings'],
    'Betty': ['Cheese', 'Soda', 'Tortilla', 'Wings'],
    'Carl': ['Cheese', 'Ice Cream', 'Wings'],
    'Danny': ['Cheese', 'Ice Cream', 'Salsa', 'Soda', 'Tortilla'],
    'Emily': ['Salsa', 'Soda', 'Tortilla', 'Wings'],
    'Frank': ['Cheese', 'Cracker', 'Ice Cream', 'Wings']
}

# Convert friend_items to list of lists for transaction encoding
transactions = list(friend_items.values())

# Transaction encoding
te = TransactionEncoder()
te_ary = te.fit(transactions).transform(transactions)
df = pd.DataFrame(te_ary, columns=te.columns_)

# Apply Apriori algorithm
frequent_itemsets = apriori(df, min_support=0.3, use_colnames=True)

# Generate association rules
rules = association_rules(frequent_itemsets, metric="confidence", min_threshold=0.5)

# Filter rules for {Cheese, Wings} ==> {Soda}
filtered_rules = rules[(rules['antecedents'] == {'Cheese', 'Wings'}) & (rules['consequents'] == {'Soda'})]

# Calculate metrics for {Cheese, Wings} ==> {Soda}
confidence, lift, leverage, zhang = calculate_metrics(filtered_rules)

print('Metrics for {Cheese, Wings} ==> {Soda}:')
print('Confidence:', confidence)
print('Lift:', lift)
print('Leverage:', leverage)
print('Zhang\'s metric:', zhang)

#***********************************Question 2.a********************************
import numpy as np
import pandas as pd
import random
import sys

import matplotlib.pyplot as plt
from google.colab import files
uploaded = files.upload()
import io

def submissionDetails():
    print("""\nName : Sai Ram Oduri \n Course: CS 484 Introduction to Machine Learning\n\n""")

trainData = pd.read_csv(io.BytesIO(uploaded['Chinese_Bakery.csv']))
trainData.head()
#The table has two columns Customer & Item
#Customer i

#***********************************Question 2.a********************************
from mlxtend.preprocessing import TransactionEncoder
from mlxtend.frequent_patterns import apriori, association_rules

print('\nFit-Transform using Transaction Encoder\n')

train_data = data.groupby('Customer')['Item'].apply(set).tolist()
te = TransactionEncoder()
te_array = te.fit(trainData).transform(trainData)
df = pd.DataFrame(te_array, columns=te.columns_)

print('\nEncoded Data:\n')
print(df.head())

#***********************************Question 2.b********************************
# Calculate the number of unique items in the Universal Set
universal_items = set(item for items in train_data  for item in items)
unique_items = len(universal_items)

# Calculate the maximum number of itemsets (2^n - 1)
max_num_itemsets = 2**unique_items - 1

# Calculate the maximum number of association rules (3^n - 2^n - n)
maximum_assoc_rules = 3**unique_items - 2**unique_items - unique_items

print('Number of items in the Universal Set:', unique_items)
print('Maximum number of itemsets in theory:', max_num_itemsets)
print('Maximum number of association rules in theory:', maximum_assoc_rules)

#***********************************Question 2.c********************************
min_support_threshold = 100 / len(train_data)
freq_its_fil = frequent_itemsets[frequent_itemsets['support'] >= min_support_threshold]
largest_item_set = freq_its_fil['itemsets'].apply(lambda x: len(x)).max()

print('Number of itemsets with atleast 100 customers:',len(freq_its_fil))
print('Largest number of items (k) among these itemsets:', largest_item_set)

#***********************************Question 2.d********************************
import matplotlib.pyplot as plt

min_confidence_threshold = 0.01


filtered_rules_confidence = rules[rules['confidence'] >= min_confidence_threshold]

# Plot Support vs Confidence with a color gradient based on Lift
plt.figure(figsize=(10, 6))
scatter = plt.scatter(
    filtered_rules_confidence['support'],
    filtered_rules_confidence['confidence'],
    c=filtered_rules_confidence['lift'],
    cmap=plt.cm.get_cmap('viridis'),
    alpha=0.7
)

plt.xlabel('Support')
plt.ylabel('Confidence')
plt.title('Support vs Confidence for Association Rules')
plt.colorbar(scatter, label='Lift')
plt.legend()

plt.show()

# Number of association rules meeting the confidence threshold
num_association_rules = len(filtered_rules_confidence)
print('Number of association rules with at least 1% confidence:', num_association_rules)

# Corrected column names based on the rules DataFrame
table_columns = ['antecedents', 'consequents', 'support', 'confidence', 'conviction', 'lift']

# Display the relevant metrics in a table
display_table = sorted_rules[table_columns]

# Display the table
print(display_table)

#***********************************Question 3***********************

#***********************************Question 3.a********************************

import numpy as np
import pandas as pd
import random
import sys

import matplotlib.pyplot as plt
from google.colab import files
uploaded = files.upload()
import io
# Load the data
data = pd.read_csv(io.BytesIO(uploaded['TwoFeatures.csv']))
x1 = data['x1']
x2 = data['x2']

# Plot x2 versus x1
plt.scatter(x1, x2)
plt.xlabel('x1')
plt.ylabel('x2')
plt.grid(True)
plt.title('x2 versus x1')
plt.show()

#***********************************Question 3.b********************************

from sklearn.cluster import KMeans

X = data[['x1', 'x2']].values

# Find optimal number of clusters without any transformation
def find_optimal_clusters(X, max_clusters):
    distortions = []
    for i in range(1, max_clusters + 1):
        kmeans = KMeans(n_clusters=i, random_state=0).fit(X)
        distortion = kmeans.inertia_
        distortions.append(distortion)
    return distortions, kmeans.cluster_centers_

# Find optimal clusters without any transformation
max_clusters = 8
distortions, centroids = find_optimal_clusters(X, max_clusters)

# Plot Elbow Values vs. Number of Clusters
plt.plot(range(1, max_clusters + 1), distortions, marker='o')
plt.xlabel('Number of Clusters')
plt.ylabel('Total Within-Cluster Sum of Squares (TWCSS)')
plt.title('Elbow Method for Optimal Number of Clusters (No Transformation)')
plt.show()

# Display results in a table
table_no_transform = pd.DataFrame({
    'Number of Clusters': range(1, max_clusters + 1),
    'TWCSS': distortions
})
print('Results without any transformation:')
print(table_no_transform)

#***********************************Question 3.c********************************
# Linear rescale x1 and x2 to have a minimum of zero and a maximum of ten
x1_rescaled = 10 * (x1 - x1.min()) / (x1.max() - x1.min())
x2_rescaled = 10 * (x2 - x2.min()) / (x2.max() - x2.min())
X_rescaled = np.column_stack((x1_rescaled, x2_rescaled))

# Find optimal number of clusters with linear rescaling
distortions_rescaled, centroids_rescaled = find_optimal_clusters(X_rescaled, max_clusters)

# Plot Elbow Values vs. Number of Clusters after rescaling
plt.plot(range(1, max_clusters + 1), distortions_rescaled, marker='o')
plt.xlabel('Number of Clusters')
plt.ylabel('Total Within-Cluster Sum of Squares (TWCSS)')
plt.title('Elbow Method for Optimal Number of Clusters (Linear Rescaling)')
plt.show()

# Display results in a table after rescaling
table_rescaled = pd.DataFrame({
    'Number of Clusters': range(1, max_clusters + 1),
    'TWCSS': distortions_rescaled
})
print('\nResults with linear rescaling:')
print(table_rescaled)

# Inverse transform the centroids to the original scale
centroids_original_scale = centroids_rescaled * [(x1.max() - x1.min()) / 10, (x2.max() - x2.min()) / 10]

# Display centroids in the original scale
print('\nCentroids in the original scale:')
print(pd.DataFrame(centroids_original_scale, columns=['x1', 'x2']))

"""Due to the differing sizes of the features (x1 and x2), two distinct optimum cluster solutions are produced. The clustering in the first solution, which has no alteration, is based on the original scales of x1 and x2. Clustering is conducted on the rescaled features in the second solution using linear rescaling.

Clusters are produced without alteration based on the original distribution of x1 and x2. Linear rescaling alters the feature range and distribution, thereby accentuating distinct patterns in the data. As a result, the ideal number of clusters and their centroids differ between the two methods, indicating how feature scaling affects clustering outcomes.





"""